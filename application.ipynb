{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTCuQv3kIMXqR06Tmv8Xjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rin-Sanity/test1/blob/main/application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **対話エージェント作成**"
      ],
      "metadata": {
        "id": "7U2I-LnoEWGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "593fUGH_nQ22",
        "outputId": "bffdbd8e-f633-422d-912b-dd845190a130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-utils\n",
            "0 upgraded, 6 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 7,367 kB of archives.\n",
            "After this operation, 59.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmecab2 amd64 0.996-14build9 [199 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmecab-dev amd64 0.996-14build9 [306 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-utils amd64 0.996-14build9 [4,850 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-ipadic all 2.7.0-20070801+main-3 [6,718 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mecab amd64 0.996-14build9 [136 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-3 [4,384 B]\n",
            "Fetched 7,367 kB in 1s (8,862 kB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-14build9_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-14build9) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-14build9_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-14build9_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../3-mecab-ipadic_2.7.0-20070801+main-3_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-3) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../4-mecab_0.996-14build9_amd64.deb ...\n",
            "Unpacking mecab (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../5-mecab-ipadic-utf8_2.7.0-20070801+main-3_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-3) ...\n",
            "Setting up libmecab2:amd64 (0.996-14build9) ...\n",
            "Setting up libmecab-dev (0.996-14build9) ...\n",
            "Setting up mecab-utils (0.996-14build9) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-3) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-14build9) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-3) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt install mecab libmecab-dev mecab-ipadic-utf8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtGgMuPGneWB",
        "outputId": "ef156d1c-e030-43d8-a544-106cadc0aa48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.8\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=c73b855383b43041bee8880afcf52ffdad27826be61fcaf91831c372f8b4b5cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "from os.path import dirname, join, normpath\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "kMt_4POirdvx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = MeCab.Tagger()\n",
        "\n",
        "print(tagger.parse('私は私のことが好きなあなたが好きです'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PpgBVDIsS6o",
        "outputId": "b783f41e-c844-4b74-d07e-8a1a673f224d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私\tワタクシ\tワタクシ\t私-代名詞\t代名詞\t\t\t0\n",
            "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\n",
            "私\tワタクシ\tワタクシ\t私-代名詞\t代名詞\t\t\t0\n",
            "の\tノ\tノ\tの\t助詞-格助詞\t\t\t\n",
            "こと\tコト\tコト\t事\t名詞-普通名詞-一般\t\t\t2\n",
            "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\n",
            "好き\tスキ\tスキ\t好き\t形状詞-一般\t\t\t2\n",
            "な\tナ\tダ\tだ\t助動詞\t助動詞-ダ\t連体形-一般\t\n",
            "あなた\tアナタ\tアナタ\t貴方\t代名詞\t\t\t2\n",
            "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\n",
            "好き\tスキ\tスキ\t好き\t形状詞-一般\t\t\t2\n",
            "です\tデス\tデス\tです\t助動詞\t助動詞-デス\t終止形-一般\t\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    node = tagger.parseToNode(text)\n",
        "\n",
        "    tokens = []\n",
        "    while node:\n",
        "        if node.surface != '':\n",
        "            tokens.append(node.surface)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "4tnp2WkLtBZ4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize('私は私のことが好きなあなたが好きです')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9CicDOUt5O1",
        "outputId": "bad5ae58-7a1e-4835-f825-a54934503671"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['私', 'は', '私', 'の', 'こと', 'が', '好き', 'な', 'あなた', 'が', '好き', 'です']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "print(tagger.parse('私は私のことが好きなあなたが好きです'))\n",
        "\n",
        "assert tagger.parse('私は私のことが好きなあなたが好きです') == '私 は 私 の こと が 好き な あなた が 好き です \\n'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Tp7TeIuH4W",
        "outputId": "d54706dd-10cd-4ecc-ec84-50cb97ec3bb7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 私 の こと が 好き な あなた が 好き です \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return tagger.parse(text).strip().split(' ')\n"
      ],
      "metadata": {
        "id": "HIgbahl5uQ1A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト12**"
      ],
      "metadata": {
        "id": "Vkt2OWiq43Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def calc_bow(tokenized_texts):  # <2>\n",
        "    # Build vocabulary <3>\n",
        "    vocabulary = {}\n",
        "    for tokenized_text in tokenized_texts:\n",
        "        for token in tokenized_text:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary[token] = len(vocabulary)\n",
        "\n",
        "    n_vocab = len(vocabulary)\n",
        "\n",
        "    # Build BoW Feature Vector <4>\n",
        "    bow = [[0] * n_vocab for i in range(len(tokenized_texts))]\n",
        "    for i, tokenized_text in enumerate(tokenized_texts):\n",
        "        for token in tokenized_text:\n",
        "            index = vocabulary[token]\n",
        "            bow[i][index] += 1\n",
        "\n",
        "    return vocabulary, bow\n",
        "\n",
        "\n",
        "# 入力文のlist\n",
        "texts = [\n",
        "    '私は私のことが好きなあなたが好きです',\n",
        "    '私はラーメンが好きです',\n",
        "    '富士山は日本一高い山です',\n",
        "]\n",
        "\n",
        "tokenized_texts = [tokenize(text) for text in texts]\n",
        "vocabulary, bow = calc_bow(tokenized_texts)\n"
      ],
      "metadata": {
        "id": "Sq8IV8VjvcMh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPLPdWMQ1Zyk",
        "outputId": "227dfc7c-ab1e-4d6c-dc64-6d83307cb16e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'私': 0,\n",
              " 'は': 1,\n",
              " 'の': 2,\n",
              " 'こと': 3,\n",
              " 'が': 4,\n",
              " '好き': 5,\n",
              " 'な': 6,\n",
              " 'あなた': 7,\n",
              " 'です': 8,\n",
              " 'ラーメン': 9,\n",
              " '富士': 10,\n",
              " '山': 11,\n",
              " '日本': 12,\n",
              " '一': 13,\n",
              " '高い': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6DohnUT1mpx",
        "outputId": "fed43f5f-bef0-486f-fb74-a97bdacd2cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              " [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト13**"
      ],
      "metadata": {
        "id": "OOvBn3y-4QvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calc_bow(tokenized_texts):\n",
        "    counts = [Counter(tokenized_text)\n",
        "              for tokenized_text in tokenized_texts]  # <1>\n",
        "    sum_counts = sum(counts, Counter())  # <2>\n",
        "    vocabulary = sum_counts.keys()\n",
        "\n",
        "    bow = [[count[word] for word in vocabulary]\n",
        "           for count in counts]  # <3>\n",
        "\n",
        "    return bow\n",
        "\n",
        "\n",
        "# 入力文のlist\n",
        "texts = [\n",
        "    '私は私のことが好きなあなたが好きです',\n",
        "    '私はラーメンが好きです',\n",
        "    '富士山は日本一高い山です',\n",
        "]\n",
        "\n",
        "tokenized_texts = [tokenize(text) for text in texts]\n",
        "bow = calc_bow(tokenized_texts)\n"
      ],
      "metadata": {
        "id": "U522sr_04Tmu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(['A','A','A','B','B','B','C',])"
      ],
      "metadata": {
        "id": "JjuZHhQQ4fkH",
        "outputId": "2579a8a4-64e7-4c23-ba89-f01f5dbb5ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'A': 3, 'B': 3, 'C': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト14**"
      ],
      "metadata": {
        "id": "xpOkTCWs3fFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [\n",
        "    '私は私のことが好きなあなたが好きです',\n",
        "    '私はラーメンが好きです。',\n",
        "    '富士山は日本一高い山です',\n",
        "]\n",
        "\n",
        "# Bag of Words計算\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize)  # <2>\n",
        "vectorizer.fit(texts)  # <3>\n",
        "bow = vectorizer.transform(texts)  # <4>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtsmZIBa1rna",
        "outputId": "15143604-d2aa-4f5f-c0b2-f7b5641ddced"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow"
      ],
      "metadata": {
        "id": "uXgrFD6D3Fm2",
        "outputId": "9a2a2627-5718-44cf-a795-b2601502a647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x16 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 23 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow.__class__"
      ],
      "metadata": {
        "id": "MLHQZ4F63kue",
        "outputId": "b3f841aa-6368-4881-aa6a-1bea2ceb07bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow)"
      ],
      "metadata": {
        "id": "Jh1fhCdc3tE4",
        "outputId": "a70ad757-37bb-46de-cad8-47448ace8a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1\n",
            "  (0, 2)\t2\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 10)\t2\n",
            "  (0, 14)\t2\n",
            "  (1, 0)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 14)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 11)\t1\n",
            "  (2, 12)\t2\n",
            "  (2, 13)\t1\n",
            "  (2, 15)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow.toarray()"
      ],
      "metadata": {
        "id": "VjA_6gep3zhm",
        "outputId": "20dcf718-daf4-4333-9e49-52a3d51d4fc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oAEVE5Yu5DIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト16**"
      ],
      "metadata": {
        "id": "MPFxoYVb5Fy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from os.path import join\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CSVファイルのパスを直接指定\n",
        "csv_path = './training_data.csv'  # 相対パスまたは絶対パスを使用\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "training_data = pd.read_csv(csv_path)\n",
        "training_texts = training_data['text']\n",
        "\n",
        "# Bag of Words計算\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
        "vectorizer.fit(training_texts)\n",
        "bow = vectorizer.transform(training_texts)\n"
      ],
      "metadata": {
        "id": "aNhTksML79bs",
        "outputId": "b2e65f75-5986-45c2-9556-893adac1b535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト17**"
      ],
      "metadata": {
        "id": "3i-ed-aU7SbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        tokens = []\n",
        "        while node:\n",
        "            if node.surface != '':\n",
        "                tokens.append(node.surface)\n",
        "            node = node.next\n",
        "        return tokens\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前を教えてよ'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n"
      ],
      "metadata": {
        "id": "i8E3K-fq83tV",
        "outputId": "358bd168-9e34-4ee2-e23e-05ab2ff04c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0e494f40624a>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト19**"
      ],
      "metadata": {
        "id": "efTnWziBBA0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        tokens = []\n",
        "        while node:\n",
        "            if node.surface != '':\n",
        "                tokens.append(node.surface)\n",
        "            node = node.next\n",
        "        return tokens\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', CountVectorizer(tokenizer=self._tokenize)),\n",
        "            ('classifier', SVC()),\n",
        "        ])\n",
        "        pipeline.fit(texts, labels)\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def predict(self, texts):\n",
        "        return self.pipeline.predict(texts)\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前を教えてよ'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])"
      ],
      "metadata": {
        "id": "xGo4viyoBAFw",
        "outputId": "b5ff78fd-91e7-4db5-c59a-7be298fed491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f441e0c1a19a>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_class_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト21**"
      ],
      "metadata": {
        "id": "uA3kOhDJ-Ah_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "test_data_path = './test_data.csv'  # テストデータのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "predictions = dialogue_agent.predict(test_data['text'])\n",
        "\n",
        "# 精度の計算\n",
        "print(accuracy_score(test_data['label'], predictions))\n"
      ],
      "metadata": {
        "id": "Ax3-Hh9HAj3i",
        "outputId": "6811bf04-fd7c-44d6-c065-23dd7115d541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.574468085106383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **前処理**"
      ],
      "metadata": {
        "id": "GnbwOWj5EEUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X8jjye8NEQfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neologdn\n",
        "import neologdn"
      ],
      "metadata": {
        "id": "ZefQmA4f9-9O",
        "outputId": "740719e9-9533-433f-d4ca-6a646bd6d46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neologdn\n",
            "  Downloading neologdn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: neologdn\n",
            "  Building wheel for neologdn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neologdn: filename=neologdn-0.5.2-cp310-cp310-linux_x86_64.whl size=219145 sha256=a8a437786940de77a2a84e70eef56477eb169f853bf625cbe406e2f7c89230f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/87/af/2a36d74f666a8428943b70d71c5e9dd740435bf671f210672c\n",
            "Successfully built neologdn\n",
            "Installing collected packages: neologdn\n",
            "Successfully installed neologdn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(neologdn.normalize('「初めてのTensorFlow」は定価2200円+税です'))\n",
        "print(neologdn.normalize('「初めての　ＴｅｎｓｏｒＦｌｏｗ」は定価２２００円＋税です'))\n",
        "print(neologdn.normalize('｢初めての TensorFlow｣は定価2200円＋税です'))"
      ],
      "metadata": {
        "id": "LTwv1NekEDdu",
        "outputId": "d0c23b1e-4177-429e-aff5-e4afa6546fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "「初めてのTensorFlow」は定価2200円+税です\n",
            "「初めてのTensorFlow」は定価2200円+税です\n",
            "「初めてのTensorFlow」は定価2200円+税です\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#小文字化、大文字化\n",
        "text='「初めてのTensorFlow」は定価2200円+税です'\n",
        "print(text.lower())\n",
        "print(text.upper())"
      ],
      "metadata": {
        "id": "LEsGXMQhF9F_",
        "outputId": "8ec642f5-138b-4247-b091-938629808630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "「初めてのtensorflow」は定価2200円+税です\n",
            "「初めてのTENSORFLOW」は定価2200円+税です\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "normalized = unicodedata.normalize('NFKC', '㈱リックテレコム')\n",
        "\n",
        "assert normalized == '(株)リックテレコム'\n",
        "print(normalized)\n"
      ],
      "metadata": {
        "id": "q3Kk4kOXGPXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03eac4ce-8682-4dc4-b392-81396eb9ce68"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(株)リックテレコム\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "\n",
        "def normalize_and_tokenize(text):\n",
        "    normalized = unicodedata.normalize('NFKC', text)\n",
        "    return tokenize(normalized)\n",
        "\n",
        "\n",
        "texts = [\n",
        "    'ディスプレイを買った',\n",
        "    'ディスプレイを買った',\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=normalize_and_tokenize)\n",
        "vectorizer.fit(texts)\n",
        "\n",
        "print('bow:')\n",
        "print(vectorizer.transform(texts).toarray())\n",
        "\n",
        "print('vocabulary:')\n",
        "print(vectorizer.vocabulary_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-nm6b_0Hp-o",
        "outputId": "05c5ba65-5796-4057-a284-f427b2621cd5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bow:\n",
            "[[1 1 1 1]\n",
            " [1 1 1 1]]\n",
            "vocabulary:\n",
            "{'ディスプレイ': 2, 'を': 1, '買っ': 3, 'た': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト4**"
      ],
      "metadata": {
        "id": "PAVmO9q3Iuda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    node = tagger.parseToNode(text)\n",
        "    result = []\n",
        "    while node:\n",
        "        features = node.feature.split(',')  # <1>\n",
        "\n",
        "        if features[0] != 'BOS/EOS':  # <2>\n",
        "            token = features[7] if features[7] != '*' else node.surface  # <1>\n",
        "            result.append(token)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(tokenize('本を読んだ'))\n",
        "print(tokenize('本を読みました'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJfhUWQPIwdy",
        "outputId": "266363e1-2225-41d4-b8dc-993d81693ed9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本', 'を', '読む', 'た']\n",
            "['本', 'を', '読む', 'ます', 'た']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト5**"
      ],
      "metadata": {
        "id": "qD-KEIkLJrG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text, stop_words):  # <1>\n",
        "    node = tagger.parseToNode(text)\n",
        "    result = []\n",
        "    while node:\n",
        "        features = node.feature.split(',')\n",
        "\n",
        "        if features[0] != 'BOS/EOS':\n",
        "            token = features[7] if features[7] != '*' else node.surface\n",
        "            if token not in stop_words:  # <2>\n",
        "                result.append(token)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "stop_words = ['て', 'に', 'を', 'は', 'です', 'ます']  # <3>\n",
        "\n",
        "print(tokenize('本を読んだ', stop_words))\n",
        "print(tokenize('本を読みました', stop_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHtSfbHCJtGe",
        "outputId": "86bce6e9-1773-4f34-e815-ad15b20f9848"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本', '読む', 'た']\n",
            "['本', '読む', 'た']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    node = tagger.parseToNode(text)\n",
        "    result = []\n",
        "    while node:\n",
        "        features = node.feature.split(',')\n",
        "\n",
        "        if features[0] != 'BOS/EOS':\n",
        "            if features[0] not in ['助詞', '助動詞']:  # <1>\n",
        "                token = features[7] if features[7] != '*' else node.surface\n",
        "                result.append(token)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(tokenize('本を読んだ'))\n",
        "print(tokenize('本を読みました'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujR9ptQwKWFo",
        "outputId": "5d18ae11-a7c3-4514-f2d6-bb401d5af1e1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本', '読む']\n",
            "['本', '読む']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト8**"
      ],
      "metadata": {
        "id": "o01XOlwKLbVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def tokenize_numbers(text):\n",
        "    return re.sub(r'\\d+', ' SOMENUMBER ', text)\n",
        "\n",
        "\n",
        "print(tokenize_numbers('卵を1個買ったよ！'))\n",
        "print(tokenize_numbers('卵を2個買ったよ！'))\n",
        "print(tokenize_numbers('卵を10個買ったよ！'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLgOI4-qLay_",
        "outputId": "68513ee0-af00-4917-c99b-db74a1048cff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "卵を SOMENUMBER 個買ったよ！\n",
            "卵を SOMENUMBER 個買ったよ！\n",
            "卵を SOMENUMBER 個買ったよ！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト9**"
      ],
      "metadata": {
        "id": "1Ogu69BsPbFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import unicodedata\n",
        "import neologdn\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "        text = neologdn.normalize(text)\n",
        "        text = text.lower()\n",
        "\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        result = []\n",
        "        while node:\n",
        "            features = node.feature.split(',')\n",
        "\n",
        "            if features[0] != 'BOS/EOS':\n",
        "                if features[0] not in ['助詞', '助動詞']:\n",
        "                    token = features[5] if features[5] != '*' else node.surface\n",
        "                    result.append(token)\n",
        "\n",
        "            node = node.next\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "\n",
        "# 以下のコードは Jupyter Notebook や Google Colab で実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前は？'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "RF7yINyfPeHv",
        "outputId": "b1a47a4c-0db0-44b7-b246-445f4804b6fa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-85bdd2a7c371>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "isort:skip_file\n",
        "\"\"\"\n",
        "from os.path import normpath, dirname, join\n",
        "import MeCab\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import neologdn\n",
        "\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        text = unicodedata.normalize('NFKC', text)  # <1>\n",
        "        text = neologdn.normalize(text)  # <2>\n",
        "        text = text.lower()  # <3>\n",
        "\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        result = []\n",
        "        while node:\n",
        "            features = node.feature.split(',')\n",
        "\n",
        "            if features[0] != 'BOS/EOS':\n",
        "                if features[0] not in ['助詞', '助動詞']:  # <4>\n",
        "                    token = features[7] \\\n",
        "                            if features[5] != '*' \\\n",
        "                            else node.surface  # <5>\n",
        "                    result.append(token)\n",
        "\n",
        "            node = node.next\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)  # <2>\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        # <3>\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "    input_text = '名前は？'\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n",
        "\n",
        "    while True:\n",
        "        input_text = input()\n",
        "        predictions = dialogue_agent.predict([input_text])\n",
        "        predicted_class_id = predictions[0]\n",
        "\n",
        "        print(replies[predicted_class_id])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "j5XhOJEiNW6u",
        "outputId": "dd705ff6-fbbc-4ada-dbd0-f8ed8782d6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n",
            "スポーツ\n",
            "こんにちは\n",
            "スポーツしたい\n",
            "良かったですね！\n",
            "お昼食べたい\n",
            "こんにちは\n",
            "名前は\n",
            "私は〇〇といいます\n",
            "何が好き\n",
            "何か一緒に食べましょう。\n",
            "好きなものは\n",
            "えっ、ありがとうございます\n",
            "プレゼントあげます\n",
            "えっ、ありがとうございます\n",
            "これどうぞ\n",
            "こんにちは\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-f9ca33b9ce45>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "test_data_path = './test_data.csv'  # テストデータのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "predictions = dialogue_agent.predict(test_data['text'])\n",
        "\n",
        "# 精度の計算\n",
        "print(accuracy_score(test_data['label'], predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyLKeEBpQov5",
        "outputId": "71a2c804-308a-4d1c-c619-afbdab811da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6702127659574468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **形態素解析とわかち描き**"
      ],
      "metadata": {
        "id": "w-OVX2dzUuik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト1**"
      ],
      "metadata": {
        "id": "WLnA-9BOVO1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh-lGNm6V7ZS",
        "outputId": "9c284593-4b48-4463-9744-c3c5794c7a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mecab-ipadic-neologd'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 75 (delta 5), reused 54 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (75/75), 58.09 MiB | 29.04 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WEU3roqWpDZ",
        "outputId": "9865631b-2876-48a5-bf13-391aac4e5d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[install-mecab-ipadic-NEologd] : Start..\n",
            "[install-mecab-ipadic-NEologd] : Check the existance of libraries\n",
            "[install-mecab-ipadic-NEologd] :     find => ok\n",
            "[install-mecab-ipadic-NEologd] :     sort => ok\n",
            "[install-mecab-ipadic-NEologd] :     head => ok\n",
            "[install-mecab-ipadic-NEologd] :     cut => ok\n",
            "[install-mecab-ipadic-NEologd] :     egrep => ok\n",
            "[install-mecab-ipadic-NEologd] :     mecab => ok\n",
            "[install-mecab-ipadic-NEologd] :     mecab-config => ok\n",
            "[install-mecab-ipadic-NEologd] :     make => ok\n",
            "[install-mecab-ipadic-NEologd] :     curl => ok\n",
            "[install-mecab-ipadic-NEologd] :     sed => ok\n",
            "[install-mecab-ipadic-NEologd] :     cat => ok\n",
            "[install-mecab-ipadic-NEologd] :     diff => ok\n",
            "[install-mecab-ipadic-NEologd] :     tar => ok\n",
            "[install-mecab-ipadic-NEologd] :     unxz => ok\n",
            "[install-mecab-ipadic-NEologd] :     xargs => ok\n",
            "[install-mecab-ipadic-NEologd] :     grep => ok\n",
            "[install-mecab-ipadic-NEologd] :     iconv => ok\n",
            "[install-mecab-ipadic-NEologd] :     patch => ok\n",
            "[install-mecab-ipadic-NEologd] :     which => ok\n",
            "[install-mecab-ipadic-NEologd] :     file => ok\n",
            "[install-mecab-ipadic-NEologd] :     openssl => ok\n",
            "[install-mecab-ipadic-NEologd] :     awk => ok\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : mecab-ipadic-NEologd is already up-to-date\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : mecab-ipadic-NEologd will be install to /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Make mecab-ipadic-NEologd\n",
            "[make-mecab-ipadic-NEologd] : Start..\n",
            "[make-mecab-ipadic-NEologd] : Check local seed directory\n",
            "[make-mecab-ipadic-NEologd] : Check local seed file\n",
            "[make-mecab-ipadic-NEologd] : Check local build directory\n",
            "[make-mecab-ipadic-NEologd] : create /content/mecab-ipadic-neologd/libexec/../build\n",
            "[make-mecab-ipadic-NEologd] : Download original mecab-ipadic file\n",
            "[make-mecab-ipadic-NEologd] : Try to access to https://ja.osdn.net\n",
            "[make-mecab-ipadic-NEologd] : Try to download from https://ja.osdn.net/frs/g_redir.php?m=kent&f=mecab%2Fmecab-ipadic%2F2.7.0-20070801%2Fmecab-ipadic-2.7.0-20070801.tar.gz\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   122  100   122    0     0   1202      0 --:--:-- --:--:-- --:--:--  1196\n",
            "\n",
            "Hash value of /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801.tar.gz don't match\n",
            "[make-mecab-ipadic-NEologd] : Try to download from https://drive.google.com/uc?export=download&id=0B4y35FiV1wh7MWVlSDBCSXZMTXM\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 11.6M  100 11.6M    0     0  9027k      0  0:00:01  0:00:01 --:--:-- 9027k\n",
            "Hash value of /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801.tar.gz matched\n",
            "[make-mecab-ipadic-NEologd] : Decompress original mecab-ipadic file\n",
            "mecab-ipadic-2.7.0-20070801/\n",
            "mecab-ipadic-2.7.0-20070801/README\n",
            "mecab-ipadic-2.7.0-20070801/AUTHORS\n",
            "mecab-ipadic-2.7.0-20070801/COPYING\n",
            "mecab-ipadic-2.7.0-20070801/ChangeLog\n",
            "mecab-ipadic-2.7.0-20070801/INSTALL\n",
            "mecab-ipadic-2.7.0-20070801/Makefile.am\n",
            "mecab-ipadic-2.7.0-20070801/Makefile.in\n",
            "mecab-ipadic-2.7.0-20070801/NEWS\n",
            "mecab-ipadic-2.7.0-20070801/aclocal.m4\n",
            "mecab-ipadic-2.7.0-20070801/config.guess\n",
            "mecab-ipadic-2.7.0-20070801/config.sub\n",
            "mecab-ipadic-2.7.0-20070801/configure\n",
            "mecab-ipadic-2.7.0-20070801/configure.in\n",
            "mecab-ipadic-2.7.0-20070801/install-sh\n",
            "mecab-ipadic-2.7.0-20070801/missing\n",
            "mecab-ipadic-2.7.0-20070801/mkinstalldirs\n",
            "mecab-ipadic-2.7.0-20070801/Adj.csv\n",
            "mecab-ipadic-2.7.0-20070801/Adnominal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Adverb.csv\n",
            "mecab-ipadic-2.7.0-20070801/Auxil.csv\n",
            "mecab-ipadic-2.7.0-20070801/Conjunction.csv\n",
            "mecab-ipadic-2.7.0-20070801/Filler.csv\n",
            "mecab-ipadic-2.7.0-20070801/Interjection.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.adjv.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.adverbal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.demonst.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.nai.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.name.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.number.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.org.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.others.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.place.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.proper.csv\n",
            "mecab-ipadic-2.7.0-20070801/Noun.verbal.csv\n",
            "mecab-ipadic-2.7.0-20070801/Others.csv\n",
            "mecab-ipadic-2.7.0-20070801/Postp-col.csv\n",
            "mecab-ipadic-2.7.0-20070801/Postp.csv\n",
            "mecab-ipadic-2.7.0-20070801/Prefix.csv\n",
            "mecab-ipadic-2.7.0-20070801/Suffix.csv\n",
            "mecab-ipadic-2.7.0-20070801/Symbol.csv\n",
            "mecab-ipadic-2.7.0-20070801/Verb.csv\n",
            "mecab-ipadic-2.7.0-20070801/char.def\n",
            "mecab-ipadic-2.7.0-20070801/feature.def\n",
            "mecab-ipadic-2.7.0-20070801/left-id.def\n",
            "mecab-ipadic-2.7.0-20070801/matrix.def\n",
            "mecab-ipadic-2.7.0-20070801/pos-id.def\n",
            "mecab-ipadic-2.7.0-20070801/rewrite.def\n",
            "mecab-ipadic-2.7.0-20070801/right-id.def\n",
            "mecab-ipadic-2.7.0-20070801/unk.def\n",
            "mecab-ipadic-2.7.0-20070801/dicrc\n",
            "mecab-ipadic-2.7.0-20070801/RESULT\n",
            "[make-mecab-ipadic-NEologd] : Configure custom system dictionary on /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801-neologd-20200910\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for working aclocal-1.4... missing\n",
            "checking for working autoconf... found\n",
            "checking for working automake-1.4... missing\n",
            "checking for working autoheader... found\n",
            "checking for working makeinfo... missing\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking for mecab-config... /usr/bin/mecab-config\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "[make-mecab-ipadic-NEologd] : Encode the character encoding of system dictionary resources from EUC_JP to UTF-8\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.others.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.verbal.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adnominal.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adverb.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.adverbal.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.nai.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Conjunction.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Symbol.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Auxil.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Verb.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.proper.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.adjv.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Postp.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Interjection.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Others.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Filler.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.number.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.place.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.name.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Postp-col.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.demonst.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Prefix.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Noun.org.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Adj.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./Suffix.csv\n",
            "rm ./Noun.others.csv\n",
            "rm ./Noun.verbal.csv\n",
            "rm ./Adnominal.csv\n",
            "rm ./Adverb.csv\n",
            "rm ./Noun.adverbal.csv\n",
            "rm ./Noun.nai.csv\n",
            "rm ./Conjunction.csv\n",
            "rm ./Symbol.csv\n",
            "rm ./Auxil.csv\n",
            "rm ./Verb.csv\n",
            "rm ./Noun.proper.csv\n",
            "rm ./Noun.adjv.csv\n",
            "rm ./Postp.csv\n",
            "rm ./Interjection.csv\n",
            "rm ./Others.csv\n",
            "rm ./Filler.csv\n",
            "rm ./Noun.number.csv\n",
            "rm ./Noun.place.csv\n",
            "rm ./Noun.csv\n",
            "rm ./Noun.name.csv\n",
            "rm ./Postp-col.csv\n",
            "rm ./Noun.demonst.csv\n",
            "rm ./Prefix.csv\n",
            "rm ./Noun.org.csv\n",
            "rm ./Adj.csv\n",
            "rm ./Suffix.csv\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./pos-id.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./unk.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./char.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./matrix.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./right-id.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./rewrite.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./feature.def\n",
            "./../../libexec/iconv_euc_to_utf8.sh ./left-id.def\n",
            "rm ./pos-id.def\n",
            "rm ./unk.def\n",
            "rm ./char.def\n",
            "rm ./matrix.def\n",
            "rm ./right-id.def\n",
            "rm ./rewrite.def\n",
            "rm ./feature.def\n",
            "rm ./left-id.def\n",
            "mv ./Noun.csv.utf8 ./Noun.csv\n",
            "mv ./Noun.verbal.csv.utf8 ./Noun.verbal.csv\n",
            "mv ./Noun.adjv.csv.utf8 ./Noun.adjv.csv\n",
            "mv ./pos-id.def.utf8 ./pos-id.def\n",
            "mv ./Postp.csv.utf8 ./Postp.csv\n",
            "mv ./Adnominal.csv.utf8 ./Adnominal.csv\n",
            "mv ./unk.def.utf8 ./unk.def\n",
            "mv ./Adj.csv.utf8 ./Adj.csv\n",
            "mv ./Adverb.csv.utf8 ./Adverb.csv\n",
            "mv ./Others.csv.utf8 ./Others.csv\n",
            "mv ./Auxil.csv.utf8 ./Auxil.csv\n",
            "mv ./Interjection.csv.utf8 ./Interjection.csv\n",
            "mv ./Symbol.csv.utf8 ./Symbol.csv\n",
            "mv ./rewrite.def.utf8 ./rewrite.def\n",
            "mv ./Suffix.csv.utf8 ./Suffix.csv\n",
            "mv ./matrix.def.utf8 ./matrix.def\n",
            "mv ./Noun.number.csv.utf8 ./Noun.number.csv\n",
            "mv ./Noun.org.csv.utf8 ./Noun.org.csv\n",
            "mv ./Conjunction.csv.utf8 ./Conjunction.csv\n",
            "mv ./Noun.place.csv.utf8 ./Noun.place.csv\n",
            "mv ./Verb.csv.utf8 ./Verb.csv\n",
            "mv ./right-id.def.utf8 ./right-id.def\n",
            "mv ./Prefix.csv.utf8 ./Prefix.csv\n",
            "mv ./Noun.nai.csv.utf8 ./Noun.nai.csv\n",
            "mv ./left-id.def.utf8 ./left-id.def\n",
            "mv ./feature.def.utf8 ./feature.def\n",
            "mv ./Noun.proper.csv.utf8 ./Noun.proper.csv\n",
            "mv ./Filler.csv.utf8 ./Filler.csv\n",
            "mv ./char.def.utf8 ./char.def\n",
            "mv ./Noun.others.csv.utf8 ./Noun.others.csv\n",
            "mv ./Noun.adverbal.csv.utf8 ./Noun.adverbal.csv\n",
            "mv ./Noun.name.csv.utf8 ./Noun.name.csv\n",
            "mv ./Noun.demonst.csv.utf8 ./Noun.demonst.csv\n",
            "mv ./Postp-col.csv.utf8 ./Postp-col.csv\n",
            "[make-mecab-ipadic-NEologd] : Fix yomigana field of IPA dictionary\n",
            "patching file Noun.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Verb.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.adverbal.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.others.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Prefix.csv\n",
            "patching file Suffix.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Noun.verbal.csv\n",
            "patching file Noun.name.csv\n",
            "patching file Noun.org.csv\n",
            "patching file Noun.place.csv\n",
            "patching file Noun.proper.csv\n",
            "patching file Suffix.csv\n",
            "patching file Noun.demonst.csv\n",
            "patching file Noun.csv\n",
            "patching file Noun.name.csv\n",
            "[make-mecab-ipadic-NEologd] : Copy user dictionary resource\n",
            "[make-mecab-ipadic-NEologd] : Install adverb entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adverb-dict-seed.20150623.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install interjection entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-interjection-dict-seed.20170216.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install noun orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-common-noun-ortho-variant-dict-seed.20170228.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install noun orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-proper-noun-ortho-variant-dict-seed.20161110.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install entries of orthographic variant of a noun used as verb form using /content/mecab-ipadic-neologd/libexec/../seed/neologd-noun-sahen-conn-ortho-variant-dict-seed.20160323.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Install frequent adjective orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-std-dict-seed.20151126.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Not install /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-exp-dict-seed.20151126.csv.xz\n",
            "[make-mecab-ipadic-NEologd] :     When you install neologd-adjective-exp-dict-seed.20151126.csv.xz, please set --install_adjective_exp option\n",
            "\n",
            "[make-mecab-ipadic-NEologd] : Install adjective verb orthographic variant entries using /content/mecab-ipadic-neologd/libexec/../seed/neologd-adjective-verb-dict-seed.20160324.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Not install /content/mecab-ipadic-neologd/libexec/../seed/neologd-date-time-infreq-dict-seed.20190415.csv.xz\n",
            "[make-mecab-ipadic-NEologd] :     When you install neologd-date-time-infreq-dict-seed.20190415.csv.xz, please set --install_infreq_datetime option\n",
            "\n",
            "[make-mecab-ipadic-NEologd] : Not install /content/mecab-ipadic-neologd/libexec/../seed/neologd-quantity-infreq-dict-seed.20190415.csv.xz\n",
            "[make-mecab-ipadic-NEologd] :     When you install neologd-quantity-infreq-dict-seed.20190415.csv.xz, please set --install_infreq_quantity option\n",
            "\n",
            "[make-mecab-ipadic-NEologd] : Install entries of ill formed words using /content/mecab-ipadic-neologd/libexec/../seed/neologd-ill-formed-words-dict-seed.20170127.csv.xz\n",
            "[make-mecab-ipadic-NEologd] : Re-Index system dictionary\n",
            "reading ./unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "reading ./neologd-proper-noun-ortho-variant-dict-seed.20161110.csv ... 138379\n",
            "reading ./neologd-ill-formed-words-dict-seed.20170127.csv ... 60616\n",
            "reading ./Noun.others.csv ... 153\n",
            "reading ./neologd-common-noun-ortho-variant-dict-seed.20170228.csv ... 152869\n",
            "reading ./Noun.verbal.csv ... 12150\n",
            "reading ./Adnominal.csv ... 135\n",
            "reading ./Adverb.csv ... 3032\n",
            "reading ./Noun.adverbal.csv ... 808\n",
            "reading ./neologd-adjective-verb-dict-seed.20160324.csv ... 20268\n",
            "reading ./Noun.nai.csv ... 42\n",
            "reading ./neologd-noun-sahen-conn-ortho-variant-dict-seed.20160323.csv ... 26058\n",
            "reading ./neologd-interjection-dict-seed.20170216.csv ... 4701\n",
            "reading ./Conjunction.csv ... 171\n",
            "reading ./Symbol.csv ... 208\n",
            "reading ./Auxil.csv ... 199\n",
            "reading ./Verb.csv ... 130750\n",
            "reading ./Noun.proper.csv ... 27493\n",
            "reading ./Noun.adjv.csv ... 3328\n",
            "reading ./neologd-adverb-dict-seed.20150623.csv ... 139792\n",
            "reading ./neologd-adjective-std-dict-seed.20151126.csv ... 507812\n",
            "reading ./Postp.csv ... 146\n",
            "reading ./Interjection.csv ... 252\n",
            "reading ./Others.csv ... 2\n",
            "reading ./Filler.csv ... 19\n",
            "reading ./Noun.number.csv ... 42\n",
            "reading ./Noun.place.csv ... 73194\n",
            "reading ./mecab-user-dict-seed.20200910.csv ... 3224584\n",
            "reading ./Noun.csv ... 60734\n",
            "reading ./Noun.name.csv ... 34215\n",
            "reading ./Postp-col.csv ... 91\n",
            "reading ./Noun.demonst.csv ... 120\n",
            "reading ./Prefix.csv ... 224\n",
            "reading ./Noun.org.csv ... 17149\n",
            "reading ./Adj.csv ... 27210\n",
            "reading ./Suffix.csv ... 1448\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "[make-mecab-ipadic-NEologd] : Make custom system dictionary on /content/mecab-ipadic-neologd/libexec/../build/mecab-ipadic-2.7.0-20070801-neologd-20200910\n",
            "make: Nothing to be done for 'all'.\n",
            "[make-mecab-ipadic-NEologd] : Finish..\n",
            "[install-mecab-ipadic-NEologd] : OK. Let's install mecab-ipadic-NEologd.\n",
            "[install-mecab-ipadic-NEologd] : Start..\n",
            "[install-mecab-ipadic-NEologd] : /usr/lib/x86_64-linux-gnu/mecab/dic isn't current user's directory\n",
            "[install-mecab-ipadic-NEologd] : Sudo make install to /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "make[1]: Entering directory '/content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200910'\n",
            "make[1]: Nothing to be done for 'install-exec-am'.\n",
            "/bin/bash ./mkinstalldirs /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "mkdir /usr/lib/x86_64-linux-gnu/mecab\n",
            "mkdir /usr/lib/x86_64-linux-gnu/mecab/dic\n",
            "mkdir /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            " /usr/bin/install -c -m 644 ./matrix.bin /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/matrix.bin\n",
            " /usr/bin/install -c -m 644 ./char.bin /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/char.bin\n",
            " /usr/bin/install -c -m 644 ./sys.dic /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/sys.dic\n",
            " /usr/bin/install -c -m 644 ./unk.dic /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/unk.dic\n",
            " /usr/bin/install -c -m 644 ./left-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/left-id.def\n",
            " /usr/bin/install -c -m 644 ./right-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/right-id.def\n",
            " /usr/bin/install -c -m 644 ./rewrite.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/rewrite.def\n",
            " /usr/bin/install -c -m 644 ./pos-id.def /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/pos-id.def\n",
            " /usr/bin/install -c -m 644 ./dicrc /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/dicrc\n",
            "make[1]: Leaving directory '/content/mecab-ipadic-neologd/build/mecab-ipadic-2.7.0-20070801-neologd-20200910'\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Install completed.\n",
            "[install-mecab-ipadic-NEologd] : When you use MeCab, you can set '/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd' as a value of '-d' option of MeCab.\n",
            "[install-mecab-ipadic-NEologd] : Usage of mecab-ipadic-NEologd is here.\n",
            "Usage:\n",
            "    $ mecab -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd ...\n",
            "\n",
            "[install-mecab-ipadic-NEologd] : Finish..\n",
            "[install-mecab-ipadic-NEologd] : Finish..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import dirname, join, normpath\n",
        "\n",
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "MECAB_DIC_DIR = '/usr/lib/mecab/dic/mecab-ipadic-neologd'\n",
        "\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger('-d {}'.format(MECAB_DIC_DIR))\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        node = self.tagger.parseToNode(text)\n",
        "\n",
        "        tokens = []\n",
        "        while node:\n",
        "            if node.surface != '':\n",
        "                tokens.append(node.surface)\n",
        "\n",
        "            node = node.next\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)  # <2>\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        # <3>\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前を教えてよ'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "vElK1NS-VOc1",
        "outputId": "89e907ec-db27-4eb9-a661-a8e7afefac85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-f5af979e11af>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdialogue_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDialogueAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-f5af979e11af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDialogueAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMECAB_DIC_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: -d /usr/lib/mecab/dic/mecab-ipadic-neologd\ndefault dictionary path: /usr/local/lib/python3.10/dist-packages/unidic_lite/dicdir\n[ifs] no such file or directory: /usr/lib/mecab/dic/mecab-ipadic-neologd/dicrc\n----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OrmMlwVVMWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}