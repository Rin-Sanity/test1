{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1ho5jvYnEArqeCT7gbTXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rin-Sanity/test1/blob/main/application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **対話エージェント作成**"
      ],
      "metadata": {
        "id": "7U2I-LnoEWGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "593fUGH_nQ22",
        "outputId": "94d7dacb-86f0-4e96-ec0f-c6caeb992e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmecab2 mecab-ipadic mecab-utils\n",
            "The following NEW packages will be installed:\n",
            "  libmecab-dev libmecab2 mecab mecab-ipadic mecab-ipadic-utf8 mecab-utils\n",
            "0 upgraded, 6 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 7,367 kB of archives.\n",
            "After this operation, 59.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmecab2 amd64 0.996-14build9 [199 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmecab-dev amd64 0.996-14build9 [306 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-utils amd64 0.996-14build9 [4,850 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-ipadic all 2.7.0-20070801+main-3 [6,718 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 mecab amd64 0.996-14build9 [136 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-3 [4,384 B]\n",
            "Fetched 7,367 kB in 1s (6,738 kB/s)\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmecab2_0.996-14build9_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-14build9) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../1-libmecab-dev_0.996-14build9_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../2-mecab-utils_0.996-14build9_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../3-mecab-ipadic_2.7.0-20070801+main-3_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-3) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../4-mecab_0.996-14build9_amd64.deb ...\n",
            "Unpacking mecab (0.996-14build9) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../5-mecab-ipadic-utf8_2.7.0-20070801+main-3_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-3) ...\n",
            "Setting up libmecab2:amd64 (0.996-14build9) ...\n",
            "Setting up libmecab-dev (0.996-14build9) ...\n",
            "Setting up mecab-utils (0.996-14build9) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-3) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-14build9) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-3) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt install mecab libmecab-dev mecab-ipadic-utf8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtGgMuPGneWB",
        "outputId": "119778d9-a124-4915-db65-b1065472e849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=254f84b260b323eab28d6033564ff90f5dcd90bf54cca6d32817ab10bc19fb23\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "from os.path import dirname, join, normpath\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "kMt_4POirdvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = MeCab.Tagger()\n",
        "\n",
        "print(tagger.parse('私は私のことが好きなあなたが好きです'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PpgBVDIsS6o",
        "outputId": "aee70f5e-d328-4c97-91cc-022d40859f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私\tワタクシ\tワタクシ\t私-代名詞\t代名詞\t\t\t0\n",
            "は\tワ\tハ\tは\t助詞-係助詞\t\t\t\n",
            "私\tワタクシ\tワタクシ\t私-代名詞\t代名詞\t\t\t0\n",
            "の\tノ\tノ\tの\t助詞-格助詞\t\t\t\n",
            "こと\tコト\tコト\t事\t名詞-普通名詞-一般\t\t\t2\n",
            "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\n",
            "好き\tスキ\tスキ\t好き\t形状詞-一般\t\t\t2\n",
            "な\tナ\tダ\tだ\t助動詞\t助動詞-ダ\t連体形-一般\t\n",
            "あなた\tアナタ\tアナタ\t貴方\t代名詞\t\t\t2\n",
            "が\tガ\tガ\tが\t助詞-格助詞\t\t\t\n",
            "好き\tスキ\tスキ\t好き\t形状詞-一般\t\t\t2\n",
            "です\tデス\tデス\tです\t助動詞\t助動詞-デス\t終止形-一般\t\n",
            "EOS\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    node = tagger.parseToNode(text)\n",
        "\n",
        "    tokens = []\n",
        "    while node:\n",
        "        if node.surface != '':\n",
        "            tokens.append(node.surface)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "4tnp2WkLtBZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize('私は私のことが好きなあなたが好きです')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9CicDOUt5O1",
        "outputId": "eb4fe3f7-083b-49d1-ee4d-fea74b28fcc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['私', 'は', '私', 'の', 'こと', 'が', '好き', 'な', 'あなた', 'が', '好き', 'です']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "print(tagger.parse('私は私のことが好きなあなたが好きです'))\n",
        "\n",
        "assert tagger.parse('私は私のことが好きなあなたが好きです') == '私 は 私 の こと が 好き な あなた が 好き です \\n'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5Tp7TeIuH4W",
        "outputId": "1fae4d32-7f5f-422c-f19c-6f608b0c18a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 私 の こと が 好き な あなた が 好き です \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return tagger.parse(text).strip().split(' ')\n"
      ],
      "metadata": {
        "id": "HIgbahl5uQ1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト12**"
      ],
      "metadata": {
        "id": "Vkt2OWiq43Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def calc_bow(tokenized_texts):  # <2>\n",
        "    # Build vocabulary <3>\n",
        "    vocabulary = {}\n",
        "    for tokenized_text in tokenized_texts:\n",
        "        for token in tokenized_text:\n",
        "            if token not in vocabulary:\n",
        "                vocabulary[token] = len(vocabulary)\n",
        "\n",
        "    n_vocab = len(vocabulary)\n",
        "\n",
        "    # Build BoW Feature Vector <4>\n",
        "    bow = [[0] * n_vocab for i in range(len(tokenized_texts))]\n",
        "    for i, tokenized_text in enumerate(tokenized_texts):\n",
        "        for token in tokenized_text:\n",
        "            index = vocabulary[token]\n",
        "            bow[i][index] += 1\n",
        "\n",
        "    return vocabulary, bow\n",
        "\n",
        "\n",
        "# 入力文のlist\n",
        "texts = [\n",
        "    '私は私のことが好きなあなたが好きです',\n",
        "    '私はラーメンが好きです',\n",
        "    '富士山は日本一高い山です',\n",
        "]\n",
        "\n",
        "tokenized_texts = [tokenize(text) for text in texts]\n",
        "vocabulary, bow = calc_bow(tokenized_texts)\n"
      ],
      "metadata": {
        "id": "Sq8IV8VjvcMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPLPdWMQ1Zyk",
        "outputId": "6d29e8d0-f910-4071-a9f1-0ae63913066d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'私': 0,\n",
              " 'は': 1,\n",
              " 'の': 2,\n",
              " 'こと': 3,\n",
              " 'が': 4,\n",
              " '好き': 5,\n",
              " 'な': 6,\n",
              " 'あなた': 7,\n",
              " 'です': 8,\n",
              " 'ラーメン': 9,\n",
              " '富士': 10,\n",
              " '山': 11,\n",
              " '日本': 12,\n",
              " '一': 13,\n",
              " '高い': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6DohnUT1mpx",
        "outputId": "fed43f5f-bef0-486f-fb74-a97bdacd2cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              " [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト13**"
      ],
      "metadata": {
        "id": "OOvBn3y-4QvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calc_bow(tokenized_texts):\n",
        "    counts = [Counter(tokenized_text)\n",
        "              for tokenized_text in tokenized_texts]  # <1>\n",
        "    sum_counts = sum(counts, Counter())  # <2>\n",
        "    vocabulary = sum_counts.keys()\n",
        "\n",
        "    bow = [[count[word] for word in vocabulary]\n",
        "           for count in counts]  # <3>\n",
        "\n",
        "    return bow\n",
        "\n",
        "\n",
        "# 入力文のlist\n",
        "texts = [\n",
        "    '私は私のことが好きなあなたが好きです',\n",
        "    '私はラーメンが好きです',\n",
        "    '富士山は日本一高い山です',\n",
        "]\n",
        "\n",
        "tokenized_texts = [tokenize(text) for text in texts]\n",
        "bow = calc_bow(tokenized_texts)\n"
      ],
      "metadata": {
        "id": "U522sr_04Tmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(['A','A','A','B','B','B','C',])"
      ],
      "metadata": {
        "id": "JjuZHhQQ4fkH",
        "outputId": "1973665c-7ed6-462e-c7fd-14e7a08f9f25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'A': 3, 'B': 3, 'C': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト14**"
      ],
      "metadata": {
        "id": "xpOkTCWs3fFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [\n",
        "    '私は私のことが好きなあなたが好きです',\n",
        "    '私はラーメンが好きです。',\n",
        "    '富士山は日本一高い山です',\n",
        "]\n",
        "\n",
        "# Bag of Words計算\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize)  # <2>\n",
        "vectorizer.fit(texts)  # <3>\n",
        "bow = vectorizer.transform(texts)  # <4>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtsmZIBa1rna",
        "outputId": "00b5f343-70f9-46cd-a7b6-bd9539c47746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow"
      ],
      "metadata": {
        "id": "uXgrFD6D3Fm2",
        "outputId": "aa41c2ba-eff0-49ad-a0ba-93103a9c37ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x16 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 23 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow.__class__"
      ],
      "metadata": {
        "id": "MLHQZ4F63kue",
        "outputId": "ee9bd878-0f66-4aaa-d550-8175f1c3a076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow)"
      ],
      "metadata": {
        "id": "Jh1fhCdc3tE4",
        "outputId": "370b1602-9576-4e97-d169-c47a93768890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1\n",
            "  (0, 2)\t2\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 10)\t2\n",
            "  (0, 14)\t2\n",
            "  (1, 0)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 14)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 9)\t1\n",
            "  (2, 11)\t1\n",
            "  (2, 12)\t2\n",
            "  (2, 13)\t1\n",
            "  (2, 15)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow.toarray()"
      ],
      "metadata": {
        "id": "VjA_6gep3zhm",
        "outputId": "290eee64-7278-4a3d-da4d-bbb72c10a4b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 2, 0],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oAEVE5Yu5DIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト16**"
      ],
      "metadata": {
        "id": "MPFxoYVb5Fy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from os.path import join\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# CSVファイルのパスを直接指定\n",
        "csv_path = './training_data.csv'  # 相対パスまたは絶対パスを使用\n",
        "\n",
        "# CSVファイルの読み込み\n",
        "training_data = pd.read_csv(csv_path)\n",
        "training_texts = training_data['text']\n",
        "\n",
        "# Bag of Words計算\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
        "vectorizer.fit(training_texts)\n",
        "bow = vectorizer.transform(training_texts)\n"
      ],
      "metadata": {
        "id": "aNhTksML79bs",
        "outputId": "47d85af1-d26c-4861-f2ef-50e7cac4c71e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト17**"
      ],
      "metadata": {
        "id": "3i-ed-aU7SbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        tokens = []\n",
        "        while node:\n",
        "            if node.surface != '':\n",
        "                tokens.append(node.surface)\n",
        "            node = node.next\n",
        "        return tokens\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前を教えてよ'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n"
      ],
      "metadata": {
        "id": "i8E3K-fq83tV",
        "outputId": "dd4474ab-310e-4124-c81d-fef95eba51c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n",
            "あ\n",
            "こんにちは\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-0e494f40624a>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト19**"
      ],
      "metadata": {
        "id": "efTnWziBBA0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        tokens = []\n",
        "        while node:\n",
        "            if node.surface != '':\n",
        "                tokens.append(node.surface)\n",
        "            node = node.next\n",
        "        return tokens\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        pipeline = Pipeline([\n",
        "            ('vectorizer', CountVectorizer(tokenizer=self._tokenize)),\n",
        "            ('classifier', SVC()),\n",
        "        ])\n",
        "        pipeline.fit(texts, labels)\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def predict(self, texts):\n",
        "        return self.pipeline.predict(texts)\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前を教えてよ'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])"
      ],
      "metadata": {
        "id": "xGo4viyoBAFw",
        "outputId": "a4fcb388-7728-4c72-cb57-8c8363f0ffc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n",
            "あ\n",
            "こんにちは\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f441e0c1a19a>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_class_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト21**"
      ],
      "metadata": {
        "id": "uA3kOhDJ-Ah_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "test_data_path = './test_data.csv'  # テストデータのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "predictions = dialogue_agent.predict(test_data['text'])\n",
        "\n",
        "# 精度の計算\n",
        "print(accuracy_score(test_data['label'], predictions))\n"
      ],
      "metadata": {
        "id": "Ax3-Hh9HAj3i",
        "outputId": "0339746b-81da-4959-e405-d76e40e5cf2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.574468085106383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **前処理**"
      ],
      "metadata": {
        "id": "GnbwOWj5EEUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X8jjye8NEQfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neologdn\n",
        "import neologdn"
      ],
      "metadata": {
        "id": "ZefQmA4f9-9O",
        "outputId": "d387a6d8-eb24-4200-b62b-8433b03814ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neologdn\n",
            "  Downloading neologdn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: neologdn\n",
            "  Building wheel for neologdn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neologdn: filename=neologdn-0.5.2-cp310-cp310-linux_x86_64.whl size=219148 sha256=cba004b0c4373a160344af046b6b39fb405a99f2330081c5f834a55cb2e4f6a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/87/af/2a36d74f666a8428943b70d71c5e9dd740435bf671f210672c\n",
            "Successfully built neologdn\n",
            "Installing collected packages: neologdn\n",
            "Successfully installed neologdn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(neologdn.normalize('「初めてのTensorFlow」は定価2200円+税です'))\n",
        "print(neologdn.normalize('「初めての　ＴｅｎｓｏｒＦｌｏｗ」は定価２２００円＋税です'))\n",
        "print(neologdn.normalize('｢初めての TensorFlow｣は定価2200円＋税です'))"
      ],
      "metadata": {
        "id": "LTwv1NekEDdu",
        "outputId": "c03ab923-625e-47ce-ea6b-c319428b9206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "「初めてのTensorFlow」は定価2200円+税です\n",
            "「初めてのTensorFlow」は定価2200円+税です\n",
            "「初めてのTensorFlow」は定価2200円+税です\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#小文字化、大文字化\n",
        "text='「初めてのTensorFlow」は定価2200円+税です'\n",
        "print(text.lower())\n",
        "print(text.upper())"
      ],
      "metadata": {
        "id": "LEsGXMQhF9F_",
        "outputId": "2418682b-d281-4a59-e553-057d3a36c0fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "「初めてのtensorflow」は定価2200円+税です\n",
            "「初めてのTENSORFLOW」は定価2200円+税です\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "normalized = unicodedata.normalize('NFKC', '㈱リックテレコム')\n",
        "\n",
        "assert normalized == '(株)リックテレコム'\n",
        "print(normalized)\n"
      ],
      "metadata": {
        "id": "q3Kk4kOXGPXG",
        "outputId": "f43f4b7d-236d-48b0-8eec-6704ada00047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(株)リックテレコム\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "\n",
        "def normalize_and_tokenize(text):\n",
        "    normalized = unicodedata.normalize('NFKC', text)\n",
        "    return tokenize(normalized)\n",
        "\n",
        "\n",
        "texts = [\n",
        "    'ディスプレイを買った',\n",
        "    'ディスプレイを買った',\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=normalize_and_tokenize)\n",
        "vectorizer.fit(texts)\n",
        "\n",
        "print('bow:')\n",
        "print(vectorizer.transform(texts).toarray())\n",
        "\n",
        "print('vocabulary:')\n",
        "print(vectorizer.vocabulary_)\n"
      ],
      "metadata": {
        "id": "A-nm6b_0Hp-o",
        "outputId": "7bc808a3-0c6c-4965-83f1-e10702ebb381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bow:\n",
            "[[1 1 1 1]\n",
            " [1 1 1 1]]\n",
            "vocabulary:\n",
            "{'ディスプレイ': 2, 'を': 1, '買っ': 3, 'た': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト4**"
      ],
      "metadata": {
        "id": "PAVmO9q3Iuda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    node = tagger.parseToNode(text)\n",
        "    result = []\n",
        "    while node:\n",
        "        features = node.feature.split(',')  # <1>\n",
        "\n",
        "        if features[0] != 'BOS/EOS':  # <2>\n",
        "            token = features[7] if features[7] != '*' else node.surface  # <1>\n",
        "            result.append(token)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(tokenize('本を読んだ'))\n",
        "print(tokenize('本を読みました'))\n"
      ],
      "metadata": {
        "id": "xJfhUWQPIwdy",
        "outputId": "64540736-ae77-4617-d917-24114f0168f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本', 'を', '読む', 'た']\n",
            "['本', 'を', '読む', 'ます', 'た']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト5**"
      ],
      "metadata": {
        "id": "qD-KEIkLJrG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text, stop_words):  # <1>\n",
        "    node = tagger.parseToNode(text)\n",
        "    result = []\n",
        "    while node:\n",
        "        features = node.feature.split(',')\n",
        "\n",
        "        if features[0] != 'BOS/EOS':\n",
        "            token = features[7] if features[7] != '*' else node.surface\n",
        "            if token not in stop_words:  # <2>\n",
        "                result.append(token)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "stop_words = ['て', 'に', 'を', 'は', 'です', 'ます']  # <3>\n",
        "\n",
        "print(tokenize('本を読んだ', stop_words))\n",
        "print(tokenize('本を読みました', stop_words))\n"
      ],
      "metadata": {
        "id": "tHtSfbHCJtGe",
        "outputId": "ceaf4cd1-b227-480b-9503-38cbd360d0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本', '読む', 'た']\n",
            "['本', '読む', 'た']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tagger = MeCab.Tagger()\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    node = tagger.parseToNode(text)\n",
        "    result = []\n",
        "    while node:\n",
        "        features = node.feature.split(',')\n",
        "\n",
        "        if features[0] != 'BOS/EOS':\n",
        "            if features[0] not in ['助詞', '助動詞']:  # <1>\n",
        "                token = features[7] if features[7] != '*' else node.surface\n",
        "                result.append(token)\n",
        "\n",
        "        node = node.next\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(tokenize('本を読んだ'))\n",
        "print(tokenize('本を読みました'))\n"
      ],
      "metadata": {
        "id": "ujR9ptQwKWFo",
        "outputId": "ba2d0691-1358-49a2-844f-23164e66ca79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['本', '読む']\n",
            "['本', '読む']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト8**"
      ],
      "metadata": {
        "id": "o01XOlwKLbVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def tokenize_numbers(text):\n",
        "    return re.sub(r'\\d+', ' SOMENUMBER ', text)\n",
        "\n",
        "\n",
        "print(tokenize_numbers('卵を1個買ったよ！'))\n",
        "print(tokenize_numbers('卵を2個買ったよ！'))\n",
        "print(tokenize_numbers('卵を10個買ったよ！'))\n"
      ],
      "metadata": {
        "id": "YLgOI4-qLay_",
        "outputId": "a1aae834-1546-4883-a9ed-c97987d6a974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "卵を SOMENUMBER 個買ったよ！\n",
            "卵を SOMENUMBER 個買ったよ！\n",
            "卵を SOMENUMBER 個買ったよ！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト9**"
      ],
      "metadata": {
        "id": "1Ogu69BsPbFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import unicodedata\n",
        "import neologdn\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "        text = neologdn.normalize(text)\n",
        "        text = text.lower()\n",
        "\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        result = []\n",
        "        while node:\n",
        "            features = node.feature.split(',')\n",
        "\n",
        "            if features[0] != 'BOS/EOS':\n",
        "                if features[0] not in ['助詞', '助動詞']:\n",
        "                    token = features[5] if features[5] != '*' else node.surface\n",
        "                    result.append(token)\n",
        "\n",
        "            node = node.next\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "\n",
        "# 以下のコードは Jupyter Notebook や Google Colab で実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前は？'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n"
      ],
      "metadata": {
        "id": "RF7yINyfPeHv",
        "outputId": "562c057e-047c-4d51-dda4-7f3550b14eba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n",
            "う\n",
            "こんにちは\n",
            "おはよう\n",
            "こんにちは\n",
            "おやすみ\n",
            "こんにちは\n",
            "今何時「\n",
            "こんにちは\n",
            "子守唄\n",
            "こんにちは\n",
            "スポーツしたい\n",
            "子守唄でも歌いましょうか？\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-85bdd2a7c371>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "isort:skip_file\n",
        "\"\"\"\n",
        "from os.path import normpath, dirname, join\n",
        "import MeCab\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import neologdn\n",
        "\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger()\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        text = unicodedata.normalize('NFKC', text)  # <1>\n",
        "        text = neologdn.normalize(text)  # <2>\n",
        "        text = text.lower()  # <3>\n",
        "\n",
        "        node = self.tagger.parseToNode(text)\n",
        "        result = []\n",
        "        while node:\n",
        "            features = node.feature.split(',')\n",
        "\n",
        "            if features[0] != 'BOS/EOS':\n",
        "                if features[0] not in ['助詞', '助動詞']:  # <4>\n",
        "                    token = features[7] \\\n",
        "                            if features[5] != '*' \\\n",
        "                            else node.surface  # <5>\n",
        "                    result.append(token)\n",
        "\n",
        "            node = node.next\n",
        "\n",
        "        return result\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)  # <2>\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        # <3>\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "    input_text = '名前は？'\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n",
        "\n",
        "    while True:\n",
        "        input_text = input()\n",
        "        predictions = dialogue_agent.predict([input_text])\n",
        "        predicted_class_id = predictions[0]\n",
        "\n",
        "        print(replies[predicted_class_id])\n",
        "\n"
      ],
      "metadata": {
        "id": "j5XhOJEiNW6u",
        "outputId": "dd705ff6-fbbc-4ada-dbd0-f8ed8782d6e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私は〇〇といいます\n",
            "スポーツ\n",
            "こんにちは\n",
            "スポーツしたい\n",
            "良かったですね！\n",
            "お昼食べたい\n",
            "こんにちは\n",
            "名前は\n",
            "私は〇〇といいます\n",
            "何が好き\n",
            "何か一緒に食べましょう。\n",
            "好きなものは\n",
            "えっ、ありがとうございます\n",
            "プレゼントあげます\n",
            "えっ、ありがとうございます\n",
            "これどうぞ\n",
            "こんにちは\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-f9ca33b9ce45>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mpredicted_class_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "test_data_path = './test_data.csv'  # テストデータのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "predictions = dialogue_agent.predict(test_data['text'])\n",
        "\n",
        "# 精度の計算\n",
        "print(accuracy_score(test_data['label'], predictions))\n"
      ],
      "metadata": {
        "id": "kyLKeEBpQov5",
        "outputId": "71a2c804-308a-4d1c-c619-afbdab811da6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6702127659574468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **形態素解析とわかち描き**"
      ],
      "metadata": {
        "id": "w-OVX2dzUuik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **リスト1**"
      ],
      "metadata": {
        "id": "WLnA-9BOVO1u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mh-lGNm6V7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import dirname, join, normpath\n",
        "\n",
        "import MeCab\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "MECAB_DIC_DIR = '/usr/lib/mecab/dic/mecab-ipadic-neologd'\n",
        "\n",
        "\n",
        "class DialogueAgent:\n",
        "    def __init__(self):\n",
        "        self.tagger = MeCab.Tagger('-d {}'.format(MECAB_DIC_DIR))\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        node = self.tagger.parseToNode(text)\n",
        "\n",
        "        tokens = []\n",
        "        while node:\n",
        "            if node.surface != '':\n",
        "                tokens.append(node.surface)\n",
        "\n",
        "            node = node.next\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def train(self, texts, labels):\n",
        "        vectorizer = CountVectorizer(tokenizer=self._tokenize)\n",
        "        bow = vectorizer.fit_transform(texts)  # <2>\n",
        "\n",
        "        classifier = SVC()\n",
        "        classifier.fit(bow, labels)\n",
        "\n",
        "        # <3>\n",
        "        self.vectorizer = vectorizer\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def predict(self, texts):\n",
        "        bow = self.vectorizer.transform(texts)\n",
        "        return self.classifier.predict(bow)\n",
        "\n",
        "\n",
        "# 以下のコードはJupyter NotebookやGoogle Colabで実行する場合\n",
        "# ファイルのパスは直接指定します\n",
        "# 以下のパスは例です。実際のファイルの場所に合わせて変更してください。\n",
        "\n",
        "training_data_path = './training_data.csv'  # トレーニングデータのパス\n",
        "replies_path = './replies.csv'  # 応答データのパス\n",
        "\n",
        "# トレーニングデータの読み込み\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "\n",
        "dialogue_agent = DialogueAgent()\n",
        "dialogue_agent.train(training_data['text'], training_data['label'])\n",
        "\n",
        "with open(replies_path) as f:\n",
        "    replies = f.read().split('\\n')\n",
        "\n",
        "input_text = '名前を教えてよ'\n",
        "predictions = dialogue_agent.predict([input_text])\n",
        "predicted_class_id = predictions[0]\n",
        "\n",
        "print(replies[predicted_class_id])\n",
        "while True:\n",
        "    input_text = input()\n",
        "    predictions = dialogue_agent.predict([input_text])\n",
        "    predicted_class_id = predictions[0]\n",
        "\n",
        "    print(replies[predicted_class_id])\n"
      ],
      "metadata": {
        "id": "vElK1NS-VOc1",
        "outputId": "0ebe4128-fd83-44e4-a633-6979b5dfbc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-f5af979e11af>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdialogue_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDialogueAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mdialogue_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-f5af979e11af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDialogueAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-d {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMECAB_DIC_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/MeCab/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rawargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n----------------------------------------------------------\n\nFailed initializing MeCab. Please see the README for possible solutions:\n\n    https://github.com/SamuraiT/mecab-python3#common-issues\n\nIf you are still having trouble, please file an issue here, and include the\nERROR DETAILS below:\n\n    https://github.com/SamuraiT/mecab-python3/issues\n\nissueを英語で書く必要はありません。\n\n------------------- ERROR DETAILS ------------------------\narguments: -d /usr/lib/mecab/dic/mecab-ipadic-neologd\ndefault dictionary path: /usr/local/lib/python3.10/dist-packages/unidic_lite/dicdir\n[ifs] no such file or directory: /usr/lib/mecab/dic/mecab-ipadic-neologd/dicrc\n----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OrmMlwVVMWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}